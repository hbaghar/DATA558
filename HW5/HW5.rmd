---
title: 'DATA 558: HW Assignment 5'
author: 'Hriday Baghar'
date: 'Jun 9, 2022'
output: pdf_document
---

## Instructions:
You may discuss the homework problems in small groups, but you must write up the final solutions and code yourself. Please turn in your code for the problems that involve coding. However, code without written answers will receive no credit. To receive credit, you must explain your answers and show your work. All plots should be appropriately labeled and legible, with axis labels, legends, etc., as needed.

_On this assignment, some of the problems involve random number generation. Be sure to set a random seed (using the command ${\tt set.seed()}$) before you begin._

```{r, setup, message=FALSE}
library(ISLR2)
library(MASS)
library(ggplot2)
library(e1071)
library(dplyr)
```


## 1. Suppose we have an $n \times p$ data matrix $X$, and a continuous-valued response $y \in \mathbb{R}^n$. We saw in lecture that the $m$th principal component score vector is a linear combination of the $p$ features, of the form $$z_{im} = \phi_{1m}x_{i1} + \phi_{2m}x_{i2} + ... + \phi_{pm}x_{ip} \protect\tag{1}$$ (e.g. see (12.2) and (12.4) in textbook). In principal components regression, we fit a linear model to predict $y$, but instead of using the columns of $X$ as predictors, we use the first $M$ principal component score vectors, where $M < p$.
_A note before you begin: In this problem, I will ask you to “write out an expression for a linear model.” For instance, if I asked you to write out an expression for a linear model to predict an n-vector $y$ using the columns of an $n \times p$ matrix $X$, then here’s what I’d want to see: $y_i = \beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \epsilon_i$, where $\epsilon_i$ is a mean-zero noise term._

### (a) Write out an expression for the linear model that we are fitting in principal components regression. Your answer should involve $y_i, z_{i1}, ... , z_{iM}$ , a mean-zero noise term $\epsilon_i$, and some coefficients.

$$
y_i = \beta_0 + \beta_1z_{i1}+ \beta_2z_{i2}+...+ \beta_Mz_{iM} + \varepsilon_i, \ \ \ i=1,...,n
$$

where $\beta_0,...,\beta_m$ are the regression coefficients fit using least squares and M is the number of principal components and $z_{i1},...,z_{iM}$ are the principal components for observation $i$.

### (b) Now plug in Equation 1 from this homework to your answer from (a), in order to express the principal components regression model in terms of $x_{i1}, ... , x_{ip}$.


\begin{equation}
\begin{split}
y_i = &\beta_0 + \beta_1(\phi_{11}x_{i1}+\phi_{21}x_{i2}+...+\phi_{p1}x_{ip})+... \\
&+ \beta_M(\phi_{1M}x_{i1}+\phi_{2M}x_{i2}+...+\phi_{pM}x_{ip}) + \varepsilon_i, \ \ \ i=1,...,n
\end{split}
\end{equation}


### (c) Use your answer from (b) to argue that the principal components regression model is linear in the columns of $X$.

We see that the above expression is still a linear combination of $\beta$, which is what we mean when we say that the model is linear. For example for $x_{i1}$ we have a coefficient $\beta_1\phi_{11}+\beta_2\phi_{12}+...+\phi_{1M}\beta_M$. This is still a linear combination of $\beta$, hence it is linear in the columns of $X$.

### (d) In light of your answer to (c), is the following claim true? Explain your answer. _Claim: Fitting a linear model to predict $y$ using the first $m$ principal components will yield the same fitted values as fitting a linear model to predict $y$ using the columns of $X$._

The given claim is not always true. In cases where the number of principal components is less than the number of features i.e. $M<p$ the results will not be the same. This is because, using lesser principal components will change the expression of the coefficient for each $x_{ip}$ as we can see from the expression in (c). This makes sense because each principal component is able to explain a certain percentage of variance in the data, and reducing the number of principal components in principal components regression will be fit on a dataset with reduced dimensions, giving different coefficient estimates.

\pagebreak

## 2. We saw in class that $K$-means clustering minimizes the within-cluster sum of squares, given in (12.17) of the textbook. We can better understand the meaning of the within-cluster sum of squares by looking at (12.18) of the textbook. This shows us that the within-cluster sum of squares is (up to a scaling by a factor of two) the sum of squared distances from each observation to its cluster centroid.

### (a) Show _computationally_ that (12.18) holds. You can do this by repeating this procedure a whole bunch of times:
- Simulate an $n \times p$ data matrix, as well as some clusters $C_1, ... , C_K$. (It doesn’t matter whether there are any “true clusters” in your data, nor whether $C_1, ... , C_K$ correspond to these true clusters — (12.18) is a mathematical identity that should hold no matter what.)
- Compute the left-hand side of (12.18) on this data.
- Compute the right-hand side of (12.18) on this data.
- Verify that the left- and right-hand sides are equal. (If they aren’t, then you have done something wrong!)


We define functions to repeatedly create the data matrix, calculate LHS and RHS.

```{r}
set.seed(42)
sim.data <- function(p,k,n) {
  mu <- sample(seq(-10:10), p, replace = TRUE)
  A <- matrix(runif(p^2)*2-1, ncol=p) 
  var <- t(A) %*% A
  cluster <- sample(seq(1,k), n, replace = TRUE)
  
  data <- cbind(mvrnorm(n = n, mu = mu, Sigma = var), cluster)
  
  return(data)
}
```

Now let us compute the LHS of the given equality,

```{r}
get.cluster.lhs <- function(c,data){
  sum <- 0
  x <- data[data[,ncol(data)]==c,-c(ncol(data))]
  
  for(i in 1:nrow(x)){
    for(i1 in 1:nrow(x)){
      sum <- sum + sum((x[i,]-x[i1,])^2)
    }
  }
  return(sum*1/nrow(x))
}
```

And now RHS,

```{r}
get.cluster.rhs <- function(c,data){
  sum <- 0
  x <- data[data[,ncol(data)]==c,-c(ncol(data))]
  for(i in 1:nrow(x)){
      for(j in 1:ncol(x)){
        sum <- sum + (x[i,j] - mean(x[,j]))^2
      }
  }
  return(sum*2)
}
```


Using the above 3 functions we repeat the experiment 1000 times.

```{r}
perform.experiment <- function(){
  n <- sample(50:100, 1)
  p <- sample(5:10, 1)
  k <- sample(2:5, 1)
  
  data <- sim.data(p,k,n)
  
  lhs <- sapply(1:k, function(x) get.cluster.lhs(x,data))
  rhs <- as.vector(sapply(1:k, function(x) get.cluster.rhs(x,data)))
  
  return(all.equal(lhs,rhs))
}
results <- replicate(1000, perform.experiment())
#If below result is true then we know the equality holds
all.equal(results, rep(TRUE,1000))
```

### (b) _Extra Credit_: Show _analytically_ that (12.18) holds. In other words, use algebra to prove (12.18).

\pagebreak

## 3.  In this problem, you will generate simulated data, and then perform PCA and $K$-means clustering on the data.

### (a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.

```{r}
set.seed(42)
mu <- c(0,2,4)
var <- matrix(c(10, 0.5, 0.7, 0.5, 20, 0.6, 0.7, 0.6, 40), 3,3)

raw.data <- mvrnorm(n = 1000, mu = mu, Sigma = var)

data <- data.frame(rbind(
  matrix(raw.data[,1], nrow=20, ncol=50),
  matrix(raw.data[,2], nrow=20, ncol=50),
  matrix(raw.data[,3], nrow=20, ncol=50)
  ),
  class = cbind(c(rep(1,20), rep(2,20), rep(3,20)))
)
```


### (b) Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.

```{r}
pca <- prcomp(data[,-c(51)])

pca.data <- data.frame(PC1 = pca$x[,"PC1"], PC2 = pca$x[,"PC2"], 
                       class = as.factor(data$class))

ggplot(data = pca.data, aes(x=PC1, y=PC2, color=class))+
  geom_point()+
  labs(title="Scatterplot of Principal Component 1 vs Principal Component 2")
```


### (c) Perform $K$-means clustering of the observations with $K$ = 3. How well do the clusters that you obtained in $K$-means clustering compare to the true class labels?
_Hint: You can use the ${\tt table}$ function in ${\tt R}$ to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: $K$-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same._  

We define a function to display the result table and plot the first 2 principal components with the true cluster and assigned cluster to visualize the tabulated results. The colors denote the original cluster from the data generation step and the shapes denote the assigned cluster.

```{r}
get.cluster.plot <- function(K=3){
  km <- kmeans(data[,-c(51)], K, nstart = 20)
  df <- data.frame(pca.data, cluster = as.factor(km$cluster))
  print(table(km$cluster, data[,"class"]))
  ggplot(df, aes(x=PC1, y=PC2, shape = as.factor(cluster), color = class))+
    geom_point()+
    labs(title="Results of K-Means Clustering", subtitle = paste("K",K,sep="="))
}
get.cluster.plot()
```

We see that one cluster has all 20 observations correctly classified. The other 2 clusters have 10 and 8 misclassifications respectively. The algorithm does not do a good job of identifying the green cluster. The blue cluster is also not captured very well.

### (d) Perform $K$-means clustering with $K$ = 2. Describe your results.

```{r}
get.cluster.plot(2)
```

The algorithm counts most green cluster observations in the red cluster (18 of them) and the remaining 2 go in the blue cluster.

### (e) Now perform $K$-means clustering with $K$ = 4, and describe your results.

```{r}
get.cluster.plot(4)
```

Again, the red cluster (with lower variance) is well captured. The extra cluster that the algorithm determines has only 6 observations. The green and blue clusters are once again only partially identified. The blue cluster also appears to have a strange separation which appears to be non-linear in the 2D PCA plot (note the blue triangles and squares).

### (f) Now perform $K$-means clustering with $K$ = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform $K$-means clustering on the 60 × 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.

```{r}
km.2f <- kmeans(pca.data[,-c(3)], 3, nstart = 20)
table(km.2f$cluster, data[,"class"])

pca.data <- data.frame(pca.data, cluster = as.factor(km.2f$cluster))

ggplot(data = pca.data, aes(x=PC1, y=PC2, color=class, shape=cluster))+
  geom_point()+
  labs(title="Results of K-Means Clustering on Scaled Data")
```

Using the first two Principal components we see that there is an improvement in identifying the green cluster.

### (g) Using the scale function, perform $K$-means clustering with $K$ = 3 on the data _after scaling each variable to have standard deviation one_. How do these results compare to those obtained in (b)? Explain

```{r}
km.2g <- kmeans(scale(data[,-c(51)]), 3, nstart = 20)
table(km.2g$cluster, data[,"class"])


pca <- prcomp(data.frame(scale(data[,-c(51)])))


pca.data <- data.frame(PC1 = pca$x[,"PC1"], PC2 = pca$x[,"PC2"], 
                       class = as.factor(data$class),
                       cluster = as.factor(km.2g$cluster))

ggplot(data = pca.data, aes(x=PC1, y=PC2, color=class, shape=cluster))+
  geom_point()+
  labs(title="Results of K-Means Clustering on Scaled Data")
```

Applying K-means on the scaled data does a better job of identifying the blue cluster and perhaps also the green cluster. It is worth noting that the blue cluster is the one with the largest mean and variance and this result is consistent with the idea of what scaling tries to achieve.

\pagebreak

## 4. This problem involves the ${\tt OJ}$ data set, which is part of the ${\tt ISLR2}$ package. 

### (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r}
set.seed(42)
train <- sample(nrow(OJ), size = 800, replace = FALSE)
```

### (b) Fit a support vector classifier to the training data using ${\tt cost=0.01}$, with ${\tt Purchase}$ as the response and the other variables as predictors. Use the ${\tt summary()}$ function to produce summary statistics, and describe the results obtained.

```{r}
svm.4b <- svm(Purchase ~ ., data = OJ, subset = train, cost = 0.01, kernel = "linear")
summary(svm.4b)
```

There were 432 support vectors in this model out of which 215 were assigned to CH and 217 assigned to MM.

### (c) What are the training and test error rates?

```{r}
#Defining function to reuse
get.svm.metrics <- function(model){
  train.preds <- table(predicted = predict(model, OJ[train,]),
                       true = OJ[train, "Purchase"])
  print("Confusion matrix: Training data")
  print(train.preds)
  train.error <- round(1 - sum(diag(train.preds))/sum(train.preds),2)
  
  test.preds <- table(predict(model, OJ[-train,]), OJ[-train, "Purchase"])
  print("Confusion matrix: Test data")
  print(test.preds)
  test.error <- round(1 - sum(diag(test.preds))/sum(test.preds),2)
  
  return(c(train.error, test.error))
}

error <- get.svm.metrics(svm.4b)
```

Training error rate = `r error[1]`
Test error rate = `r error[2]`

### (d) Use the ${\tt tune()}$ function to select an optimal cost. Consider values in the range 0.01 to 10.

```{r}
tune.out <- tune(svm, Purchase ~ ., data = OJ[train,], kernel = "linear",
                 ranges = list(cost = c(0.01, 0.05, 0.1, 0.5, 1, 5, 10)))

summary(tune.out)
tune.out$best.parameters$cost
```

The best cost to use is `r tune.out$best.parameters$cost`.

### (e) Compute the training and test error rates using this new value for ${\tt cost}$.

```{r}
svm.4e <- svm(Purchase ~ ., data = OJ, kernel = "linear", 
              cost = tune.out$best.parameters$cost, subset = train)
summary(svm.4e)
error <- get.svm.metrics(svm.4e)
```

The model has 331 support vectors - 166 belong to CH and 165 belong to MM.

Training error rate = `r error[1]`
Test error rate = `r error[2]`


### (f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for ${\tt gamma}$.

Using radial kernel and cost = 0.01:

```{r}
svm.4f <- svm(Purchase ~ ., data = OJ, subset = train, cost = 0.01, kernel = "radial")
summary(svm.4f)
error <- get.svm.metrics(svm.4f)
```

Training error rate = `r error[1]`
Test error rate = `r error[2]`

Finding optimal value of cost:

```{r}
tune.out <- tune(svm, Purchase ~ ., data = OJ[train,], kernel = "radial",
                 ranges = list(cost = c(0.01, 0.05, 0.1, 0.5, 1, 5, 10)))

summary(tune.out)
```

Cost = `r tune.out$best.parameters$cost` is the optimal value. Let us fit a model using this value of cost.

```{r}
svm.4f.best <- svm(Purchase ~ ., data = OJ, subset = train, 
                   cost = tune.out$best.parameters$cost, kernel = "radial")
summary(svm.4f.best)
error <- get.svm.metrics(svm.4f.best)
```

Training error rate = `r error[1]`
Test error rate = `r error[2]`

### (g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set ${\tt degree=2}$.

```{r}
svm.4g <- svm(Purchase ~ ., data = OJ, subset = train, cost = 0.01, 
              kernel = "polynomial", degree = 2)
summary(svm.4g)
error <- get.svm.metrics(svm.4g)
```

Training error rate = `r error[1]`
Test error rate = `r error[2]`

Let us find the optimal value of cost.

```{r}
tune.out <- tune(svm, Purchase ~ ., data = OJ[train,], 
                 kernel = "polynomial", degree = 2,
                 ranges = list(cost = c(0.01, 0.05, 0.1, 0.5, 1, 5, 10)))

summary(tune.out)
```

The optimal value for cost = `r tune.out$best.parameters$cost`. Let us fit a model using this value.

```{r}
svm.4g.best <- svm(Purchase ~ ., data = OJ, subset = train,
              cost = tune.out$best.parameters$cost, kernel = "polynomial", degree = 2)
summary(svm.4g.best)
error <- get.svm.metrics(svm.4g.best)
```


Training error rate = `r error[1]`
Test error rate = `r error[2]`

### (h) Overall, which approach seems to give the best results on this data?

The SVM with a radial kernel with the best value of cost identified through cross validation gives the best result on both the training and test sets.
