---
title: "DATA 558: HW Assignment 1"
author: "Hriday Baghar"
date: "April 4, 2022"
output:
  html_notebook: default
  pdf_document:
    latex_engine: xelatex
mainfont: Helvetica
monofont: Monaco
---

### 1. Suppose that you are interested in performing regression on a particular dataset, in order to answer a particular scientific question. You need to decide whether to take a parametric or a non-parametric approach.

#### (a) In general, what are the pros and cons of taking a parametric versus a non-parametric approach?

|      |Parametric model | Non-parametric model|
|---   |-----------------|---------------------|
|Pros  | - Simplifies the problem from estimating a high-dimensional function to represent $f$ to a set of parameters <br> - Interpretability of the model is easier using parameters| - Does not assume a functional form of $f$, making it easier to fit a wider range of possible shapes for $f$ <br> - Performs well when statistical inference is not a concern, flexibility is high|
|Cons  | - The model may not be a close estimate of the true form of $f$. Hence if it is too far off from the true $f$, our estimate will be poor| - Requires a large number of observations in comparison to the number of features <br> - While flexibility is high, interpretability is low for these models|

#### (b) What properties of the data or scientific question would lead you to take a parametric approach?
1. Requirement of model interpretability: If the scientific question requires the ability to explain the relation between the response and predictors and to make statistical inferences, we should prefer parametric methods

2. Limited number of data points: If we do not have a higher number of observations, parametric methods may perform better than non-parametric methods

#### (c) What properties of the data or scientific question would lead you to take a non-parametric approach?
1. High number of data points: If we have a much larger number of observations than features in the data, we can use parametric methods

2. Requirement of model accuracy: If we are more concerned about model accuracy and do not care for interpretability, non-parametric methods can create complex models that closely estimate the true $f$.

### 2. In each setting, would you generally expect a flexible or an inflexible statistical machine learning method to perform better? Justify your answer.

#### (a) Sample size n is very small, and number of predictors p is very large.
**Inflexible will be better**. If we use a flexible model, it will overfit the training data and hence perform worse than an inflexible model on the test data.

#### (b) Sample size n is very large, and number of predictors p is very small.
**Flexible will be better**. A flexible model with a large number of observations will better estimate $f$ and the large number of observations will allow the model to generalize better for unknown data, provided that the trainingn data is representative of the data distribution.

#### (c) Relationship between predictors and response is highly non-linear.
**Flexible will be better**. A flexible model will be able to create a better estimate of the non-linearity between the response and the predictors. Inflexible methods will constrain the shapes that the estimated model can take on thereby not being able to represent this non-linearity as well as a flexible model.

#### (d) The variance of the error terms, i.e. $\sigma^2 = Var(\epsilon)$, is extremely high.
**Inflexible will be better**. If the variance of the error terms is high, that means we are running into an overfitting problem, i.e. the model is a good estimator for the training data but not so much for the test data. Using an inflexible approach will help the model better generalize the underlying relationship in the test set.

### 3. For each scenario, determine whether it is a regression or a classification problem, determine whether the goal is inference or prediction, and state the values of n (sample size) and p (number of predictors).

#### (a) I want to predict each student’s final exam score based on their homework scores. There are 50 students enrolled in the course, and each student has completed 8 homeworks.

- *Problem:* Regression
- *Goal:* Prediction
- *n:* 50
- *p:* 8

#### (b) I want to understand the factors that contribute to whether or not a student passes this course. The factors that I consider are (i) whether or not the student has previous programming experience; (ii) whether or not the student has previously studied linear algebra; (iii) whether or not the student has taken a previous stats/probability course; (iv) whether or not the student attends office hours; (v) the student’s overall GPA; (vi) the student’s year (e.g. freshman, sophomore, junior, senior, or grad student). I have data for all 50 students enrolled in the course.

- *Problem:* Classification
- *Goal:* Inference
- *n:* 50
- *p:* 6

### 4. This problem has to do with the bias-variance trade-off and related ideas, in the context of regression. For (a) and (b), it’s okay to submit hand-sketched plots: you are not supposed to actually compute the quantities referred to below on data; instead, this is a thought exercise.

#### (a) Make a plot, like the one we saw in class, with “flexibility” on the x-axis. Sketch the following curves: squared bias, variance, irreducible error, reducible error, expected prediction error. Be sure to label each curve. Indicate which level of flexibility is “best”.

#### (b) Make a plot with “flexibility” on the x-axis. Sketch curves corresponding to the training error and the test error. Be sure to label each curve. Indicate which level of flexibility is “best”.

#### (c) Describe an $\hat{f}$ that has extremely low bias, and extremely high variance. Explain your answer.
An $\hat{f}$ that passes very closely through each point in the training data will have extremely low bias and extremely high variance. This is because the bias is the difference between the true value and estimated value (which in this case will be low for the model we describe). The variance will be high because for a different training set, the $\hat{f}$ will look very different, as it depends highly on what training data is observed.

#### (d) Describe an $\hat{f}$ that has extremely high bias, and zero variance. Explain your answer.
A model that only has a randomly selected intercept value would have extremely high bias and zero variance. Bias would be extremely high because the model will have estimates that are way off from the true values. Varaince will be zero because the model always returns a single value for $\hat{f}(X_0)$ no matter what the value of $X_0$ is. Hence, $Var(\hat{f}(X_0)) = 0$.

### 5. We now consider a classification problem. Suppose we have 2 classes (labels), 25 observations per class, and p = 2 features. We will call one class the “red” class and the other class the “blue” class. The observations in the red class are drawn i.i.d. from a $N_p(\mu_r, I)$ distribution, and the observations in the blue class are drawn i.i.d. from a $N_p(\mu_b, I)$ distribution, where $\mu_r = \binom{0}{0}$ is the mean in the red class, and where $\mu_b = \binom{1.5}{1.5}$ is the mean in the blue class.

#### (a) Generate a training set, consisting of 25 observations from the red class and 25 observations from the blue class. (You will want to use the R function rnorm.) Plot the training set. Make sure that the axes are properly labeled, and that the observations are colored according to their class label.
```{r}
set.seed(558)

x.train.red <- matrix(rnorm(50), nrow = 25, ncol = 2)
x.train.blue <- matrix(rnorm(50, mean = 1.5), nrow = 25, ncol = 2)

x.train <- data.frame(rbind(x.train.red, x.train.blue), class = c(rep("red",25), rep("blue", 25)), stringsAsFactors = TRUE)

library(ggplot2)

color.names <- levels(x.train$class)
names(color.names) <- color.names 
color.scale.gg <- scale_color_manual(name = "Class", values=color.names)

ggplot(data = x.train, aes(x=X1, y=X2, color=class)) +
  geom_point() +
  color.scale.gg +
  ggtitle("Scatterplot of training set")
```


#### (b) Now generate a test set consisting of 25 observations from the red class and 25 observations from the blue class. On a single plot, display both the 2 training and test set, using one symbol to indicate training observations (e.g. circles) and another symbol to indicate the test observations (e.g. squares). Make sure that the axes are properly labeled, that the symbols for training and test observations are explained in a legend, and that the observations are colored according to their class label.
```{r}
x.test.red <- matrix(rnorm(50), nrow = 25, ncol = 2)
x.test.blue <- matrix(rnorm(50, mean = 1.5), nrow = 25, ncol = 2)

x.test <- data.frame(rbind(x.test.red, x.test.blue), class = c(rep("red",25), rep("blue", 25)), stringsAsFactors = TRUE)

shape.scale.gg <- scale_shape_manual("Dataset", values = c("training"=19, "test"=1))

ggplot() +
  geom_point(data = x.train, aes(x=X1, y=X2, color = class, shape = "training")) +
  geom_point(data = x.test, aes(x=X1, y=X2, color = class,  shape = "test")) +
  color.scale.gg +
  shape.scale.gg +
  ggtitle("Scatterplot of training and test dataset")
```


#### (c) Using the knn function in the library class, fit a k-nearest neighbors model on the training set, for a range of values of k from 1 to 20. Make a plot that displays the value of 1/k on the x-axis, and classification error (both training error and test error) on the y-axis. Make sure all axes and curves are properly labeled. Explain your results.
```{r}
library(class)

K <- seq(1,20)

y.train <- x.train[, c(3)]
x.train <- x.train[,-c(3)]

y.test <- x.test[, c(3)]
x.test <- x.test[, -c(3)]

compute.knn.metrics <- function(k){
  train.knn <- knn(train = x.train, test = x.train, cl = y.train, k = k)
  test.knn <- knn(train = x.train, test = x.test, cl = y.train, k = k)
  
  train.err <- 1 - length(which(train.knn == y.train))/nrow(x.train)
  test.err <- 1 - length(which(test.knn == y.test))/nrow(x.test)
  
  return(c(train.err, test.err, k, test.knn))
}

results <- t(sapply(K, function(k) compute.knn.metrics(k)))
error.results <- data.frame(results[,c(1:3)])
names(error.results) <- c("training.error", "test.error", "k")

ggplot(data = error.results, aes(x = 1/k)) +
  geom_line(aes(y=training.error, color="Training Error")) +
  geom_line(aes(y=test.error, color="Test Error")) +
  labs(y = "Error Rate", title = "Plot of training and test errors for KNN", subtitle = "Range of K = [1,20]") +
  theme(legend.title = element_blank()) 
```
We see from the graph that, as K decreases (1/K increases), the training error drops eventually to 0, however the corresponding test error increases. K = 1 is an example of a highly flexible model, and the graph depicts the problem with more flexibility - the tendency to overfit training data.

#### (d) For the value of k that resulted in the smallest test error in part (c) above, make a plot displaying the test observations as well as their true and predicted class labels. Make sure that all axes and points are clearly labeled.
```{r}
min.k <- which(error.results$test.error == min(error.results$test.error))
#Above value turns out to be 10

predictions <- as.factor(results[min.k, -c(1:3)])
levels(predictions) <- levels(y.test)
best.test.result <- data.frame(x.test, y.test, predictions)

shape.scale.gg <- scale_shape_manual("Predicted class", values = c("red"=19, "blue"=1)) 

ggplot(data = best.test.result, aes(x = X1, y = X2, color = y.test, shape = predictions)) +
  geom_point() +
  color.scale.gg +
  shape.scale.gg +
  ggtitle("Scatterplot of true and predicted class for test set",subtitle = "Using KNN with K = 10")
```

#### (e) Recall that the Bayes classifier assigns an observation to the red class if $Pr(Y = red|X = x) > 0.5$, and to the blue class otherwise. The Bayes error rate is the error rate associated with the Bayes classifier. What is the value of the Bayes error rate in this problem? Explain your answer.

```{r}

```


### 6. We will once again perform k-nearest-neighbors in a setting with p = 2 features. But this time, we’ll generate the data differently: let $X_1 \sim Unif[0, 1]$ and $X_2 \sim Unif[0, 1]$, i.e. the observations for each feature are i.i.d. from a uniform distribution. An observation belongs to class “red” if $(X_1 - 0.5)^2 + (X_2 - 0.5)^2 > 0.15$ and $X_1 > 0.5$; to class “green” if $(X_1 - 0.5)^2 + (X_2 - 0.5)^2 > 0.15$ and $X_1 \le 0.5$; and to class “blue” otherwise.

#### (a) Generate a training set of n = 200 observations. (You will want to use the R function runif.) Plot the training set. Make sure that the axes are properly labeled, and that the observations are colored according to their class label.

#### (b) Now generate a test set consisting of another 200 observations. On a single plot, display both the training and test set, using one symbol to indicate training observations (e.g. circles) and another symbol to indicate the test observations (e.g. squares). Make sure that the axes are properly labeled, that the symbols for training and test observations are explained in a legend, and that the observations are colored according to their class label.

#### (c) Using the knn function in the library class, fit a k-nearest neighbors model on the training set, for a range of values of k from 1 to 50. Make a plot that displays the value of 1/k on the x-axis, and classification error (both training error and test error) on the y-axis. Make sure all axes and curves are properly labeled. Explain your results.

#### (d) For the value of k that resulted in the smallest test error in part (c) above, make a plot displaying the test observations as well as their true and predicted class labels. Make sure that all axes and points are clearly labeled.

#### (e) In this example, what is the Bayes error rate? Justify your answer, and explain how it relates to your findings in (c) and (d).

### 7. This exercise involves the Boston housing data set, which is part of the ISLR2 library.

#### (a) How many rows are in this data set? How many columns? What do the rows and columns represent?

#### (b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.

#### (c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.

#### (d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.

#### (e) How many of the suburbs in this data set bound the Charles river?

#### (f) What are the mean and standard deviation of the pupil-teacher ratio among the towns in this data set?

#### (g) Which suburb of Boston has highest median value of owner-occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

#### (h) In this data set, how many of the suburbs average more than six rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.
